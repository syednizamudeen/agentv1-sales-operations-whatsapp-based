services:
  # AI Engine: Ollama with Gemma 3
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    volumes:
      - ollama_storage:/root/.ollama
    # Note: On Mac, Ollama in Docker uses CPU. For GPU, use the native Mac app instead.
    entrypoint: /bin/sh
    command: -c "ollama serve & sleep 5 && ollama pull gemma3:4b && wait"
    ports:
      - "11434:11434"

  # Database: PostgreSQL for Orders/Purchases
  db:
    image: postgres:15
    container_name: business_db
    environment:
      POSTGRES_USER: admin
      POSTGRES_PASSWORD: password123
      POSTGRES_DB: biz_data
    volumes:
      - ./db/init.sql:/docker-entrypoint-initdb.d/init.sql
      - db_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U admin -d biz_data"]
      interval: 5s
      timeout: 5s
      retries: 5

  # Backend: Node.js Orchestrator
  app:
    build: ./app
    container_name: node_agent
    depends_on:
      ollama:
        condition: service_started
      db:
        condition: service_healthy # This waits for the healthcheck above
    env_file: .env
    ports:
      - "3000:3000"
    volumes:
      - ./app:/usr/src/app
      - /usr/src/app/node_modules

  # Ngrok tunnel for public API access
  ngrok:
    image: ngrok/ngrok:latest
    container_name: ngrok_tunnel
    depends_on:
      - app
    command: http app:3000
    environment:
      NGROK_AUTHTOKEN: ${NGROK_AUTHTOKEN}
    ports:
      - "4040:4040" # Ngrok web UI

volumes:
  ollama_storage:
  db_data: